{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "3GBjrWyCxOCd",
        "outputId": "2df5f7d9-3526-42fd-9923-69fc7a8f6ff5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "17/17 - 58s - 3s/step - accuracy: 0.8381 - loss: 0.8535\n",
            "Loaded model initial validation accuracy: 0.8381\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"functional\"</span>\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1mModel: \"functional\"\u001b[0m\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃<span style=\"font-weight: bold\"> Layer (type)              </span>┃<span style=\"font-weight: bold\"> Output Shape           </span>┃<span style=\"font-weight: bold\">        Param # </span>┃<span style=\"font-weight: bold\"> Connected to           </span>┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">InputLayer</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">224</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3</span>)    │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ resnet50 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)     │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)     │     <span style=\"color: #00af00; text-decoration-color: #00af00\">23,587,712</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ densenet121 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Functional</span>)  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)     │      <span style=\"color: #00af00; text-decoration-color: #00af00\">7,037,504</span> │ image_input[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling2d  │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">2048</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ resnet50[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]         │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling2d… │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">1024</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ densenet121[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "│ (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">GlobalAveragePooling2D</span>)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Concatenate</span>) │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">3072</span>)           │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ global_average_poolin… │\n",
              "│                           │                        │                │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)             │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │        <span style=\"color: #00af00; text-decoration-color: #00af00\">786,688</span> │ concatenate[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dropout</span>)       │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">256</span>)            │              <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> │ dense[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ output (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)            │ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">7</span>)              │          <span style=\"color: #00af00; text-decoration-color: #00af00\">1,799</span> │ dropout_1[<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>][<span style=\"color: #00af00; text-decoration-color: #00af00\">0</span>]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n",
              "</pre>\n"
            ],
            "text/plain": [
              "┏━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━┓\n",
              "┃\u001b[1m \u001b[0m\u001b[1mLayer (type)             \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m       Param #\u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mConnected to          \u001b[0m\u001b[1m \u001b[0m┃\n",
              "┡━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━┩\n",
              "│ image_input (\u001b[38;5;33mInputLayer\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m224\u001b[0m, \u001b[38;5;34m3\u001b[0m)    │              \u001b[38;5;34m0\u001b[0m │ -                      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ resnet50 (\u001b[38;5;33mFunctional\u001b[0m)     │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m2048\u001b[0m)     │     \u001b[38;5;34m23,587,712\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ densenet121 (\u001b[38;5;33mFunctional\u001b[0m)  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m7\u001b[0m, \u001b[38;5;34m1024\u001b[0m)     │      \u001b[38;5;34m7,037,504\u001b[0m │ image_input[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling2d  │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m2048\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ resnet50[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]         │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ global_average_pooling2d… │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m1024\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ densenet121[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "│ (\u001b[38;5;33mGlobalAveragePooling2D\u001b[0m)  │                        │                │                        │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ concatenate (\u001b[38;5;33mConcatenate\u001b[0m) │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m3072\u001b[0m)           │              \u001b[38;5;34m0\u001b[0m │ global_average_poolin… │\n",
              "│                           │                        │                │ global_average_poolin… │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dense (\u001b[38;5;33mDense\u001b[0m)             │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │        \u001b[38;5;34m786,688\u001b[0m │ concatenate[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]      │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ dropout_1 (\u001b[38;5;33mDropout\u001b[0m)       │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m256\u001b[0m)            │              \u001b[38;5;34m0\u001b[0m │ dense[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]            │\n",
              "├───────────────────────────┼────────────────────────┼────────────────┼────────────────────────┤\n",
              "│ output (\u001b[38;5;33mDense\u001b[0m)            │ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m7\u001b[0m)              │          \u001b[38;5;34m1,799\u001b[0m │ dropout_1[\u001b[38;5;34m0\u001b[0m][\u001b[38;5;34m0\u001b[0m]        │\n",
              "└───────────────────────────┴────────────────────────┴────────────────┴────────────────────────┘\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,413,703</span> (119.83 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m31,413,703\u001b[0m (119.83 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">31,276,935</span> (119.31 MB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m31,276,935\u001b[0m (119.31 MB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">136,768</span> (534.25 KB)\n",
              "</pre>\n"
            ],
            "text/plain": [
              "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m136,768\u001b[0m (534.25 KB)\n"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "New best validation accuracy: 0.8419\n",
            "Confusion Matrix:\n",
            "[[66  2  4  1  2  0  0]\n",
            " [ 2 67  4  0  1  1  0]\n",
            " [ 3  2 53  1 15  1  0]\n",
            " [ 0  0  0 75  0  0  0]\n",
            " [ 1  3 10  0 54  4  3]\n",
            " [ 0  2  7  1 13 52  0]\n",
            " [ 0  0  0  0  0  0 75]]\n",
            "Classification Report:\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec       0.92      0.88      0.90        75\n",
            "         bcc       0.88      0.89      0.89        75\n",
            "         bkl       0.68      0.71      0.69        75\n",
            "          df       0.96      1.00      0.98        75\n",
            "         mel       0.64      0.72      0.68        75\n",
            "          nv       0.90      0.69      0.78        75\n",
            "        vasc       0.96      1.00      0.98        75\n",
            "\n",
            "    accuracy                           0.84       525\n",
            "   macro avg       0.85      0.84      0.84       525\n",
            "weighted avg       0.85      0.84      0.84       525\n",
            "\n"
          ]
        }
      ],
      "source": [
        "# version 5 : 84%\n",
        "\n",
        "##############################################\n",
        "# 1. Setup: Mount Drive, Copy Data to Local Storage & Verify Paths\n",
        "##############################################\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from google.colab import drive\n",
        "\n",
        "drive.mount('/content/drive')\n",
        "\n",
        "drive_base_path = \"/content/drive/MyDrive/SkinCancerDetection\"\n",
        "local_base_path = \"/content/SkinCancerDetection\"\n",
        "\n",
        "if not os.path.exists(local_base_path):\n",
        "    print(\"Copying dataset from Google Drive to local storage. This may take a few minutes...\")\n",
        "    get_ipython().system(f'cp -r \"{drive_base_path}\" \"{local_base_path}\"')\n",
        "else:\n",
        "    print(\"Dataset already exists in local storage.\")\n",
        "\n",
        "base_path = local_base_path\n",
        "print(f\"Using base path: {base_path}\")\n",
        "\n",
        "train_meta_file = os.path.join(base_path, \"train_metadata.csv\")\n",
        "val_meta_file   = os.path.join(base_path, \"val_metadata.csv\")\n",
        "test_meta_file  = os.path.join(base_path, \"test_metadata.csv\")\n",
        "\n",
        "for file in [train_meta_file, val_meta_file, test_meta_file]:\n",
        "    if not os.path.exists(file):\n",
        "        raise FileNotFoundError(f\"Metadata file not found: {file}\")\n",
        "    else:\n",
        "        print(f\"Found metadata file: {file}\")\n",
        "\n",
        "train_folder = os.path.join(base_path, \"train\")\n",
        "val_folder   = os.path.join(base_path, \"val\")\n",
        "test_folder  = os.path.join(base_path, \"test\")\n",
        "\n",
        "for folder in [train_folder, val_folder, test_folder]:\n",
        "    if not os.path.exists(folder):\n",
        "        raise FileNotFoundError(f\"Image folder not found: {folder}\")\n",
        "    else:\n",
        "        print(f\"Found image folder: {folder}\")\n",
        "\n",
        "##############################################\n",
        "# 2. Load & Preprocess Metadata (Simplified - Labels Only)\n",
        "##############################################\n",
        "def load_metadata(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    required_columns = ['image_id', 'dx']\n",
        "    missing = [col for col in required_columns if col not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns in {csv_path}: {missing}\")\n",
        "    return df\n",
        "\n",
        "train_df = load_metadata(train_meta_file)\n",
        "val_df   = load_metadata(val_meta_file)\n",
        "test_df  = load_metadata(test_meta_file)\n",
        "\n",
        "TEST_MODE = False\n",
        "\n",
        "if TEST_MODE:\n",
        "    train_df = train_df.sample(100, random_state=42)\n",
        "    val_df   = val_df.sample(50, random_state=42)\n",
        "    batch_size = 8\n",
        "    epochs = 2\n",
        "else:\n",
        "    batch_size = 32  # Match your original 83% setup\n",
        "    epochs = 20  # Match your original 83% run\n",
        "\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_df['dx'])\n",
        "val_labels   = label_encoder.transform(val_df['dx'])\n",
        "num_classes  = len(label_encoder.classes_)\n",
        "print(\"Number of classes:\", num_classes)\n",
        "\n",
        "##############################################\n",
        "# 3. Image Loading, Preprocessing & Basic Augmentation\n",
        "##############################################\n",
        "def load_and_preprocess_image(image_path, augment=False):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    if augment:\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "        img = tf.image.random_brightness(img, max_delta=0.1)\n",
        "        img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
        "    img = resnet_preprocess(img)\n",
        "    return img\n",
        "\n",
        "##############################################\n",
        "# 4. Data Generator (No Extra Augmentation Layers)\n",
        "##############################################\n",
        "def data_generator(df, image_folder, labels, augment=False):\n",
        "    num_samples = len(df)\n",
        "    indices = np.arange(num_samples)\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        for start in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[start:start + batch_size]\n",
        "            batch_images = []\n",
        "            batch_labels = []\n",
        "            for idx in batch_indices:\n",
        "                try:\n",
        "                    raw_filename = str(df.iloc[idx]['image_id'])\n",
        "                    filename = os.path.splitext(raw_filename)[0] + \".png\"\n",
        "                    image_path = os.path.join(image_folder, filename)\n",
        "                    if not os.path.exists(image_path):\n",
        "                        print(f\"Warning: {image_path} not found. Skipping.\")\n",
        "                        continue\n",
        "                    img = load_and_preprocess_image(image_path, augment=augment)\n",
        "                    batch_images.append(img)\n",
        "                    batch_labels.append(labels[idx])\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing index {idx}: {e}\")\n",
        "                    continue\n",
        "            if not batch_images:\n",
        "                continue\n",
        "            images_tensor = tf.stack(batch_images)\n",
        "            labels_tensor = tf.convert_to_tensor(to_categorical(batch_labels, num_classes=num_classes), dtype=tf.float32)\n",
        "            yield images_tensor, labels_tensor\n",
        "\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(train_df, train_folder, train_labels, augment=True),\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=([None, 224, 224, 3], [None, num_classes])\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(val_df, val_folder, val_labels, augment=False),\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=([None, 224, 224, 3], [None, num_classes])\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "steps_per_epoch = math.ceil(len(train_df) / batch_size)\n",
        "validation_steps = math.ceil(len(val_df) / batch_size)\n",
        "print(f\"Steps per epoch (train): {steps_per_epoch}\")\n",
        "print(f\"Steps per epoch (val): {validation_steps}\")\n",
        "\n",
        "##############################################\n",
        "# 5. Build the ResNet50 + DenseNet121 Hybrid Model (No Fine-Tuning)\n",
        "##############################################\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50, DenseNet121\n",
        "\n",
        "local_checkpoint_path = \"model_checkpoint.keras\"\n",
        "checkpoint_drive_path = \"/content/drive/MyDrive/model_checkpoint.keras\"\n",
        "\n",
        "def build_model(num_classes):\n",
        "    image_input = Input(shape=(224, 224, 3), name=\"image_input\")\n",
        "    resnet_model = ResNet50(include_top=False, weights=\"imagenet\", name=\"resnet50\")\n",
        "    densenet_model = DenseNet121(include_top=False, weights=\"imagenet\", name=\"densenet121\")\n",
        "    # No fine-tuning: keep all layers frozen\n",
        "    for layer in resnet_model.layers:\n",
        "        layer.trainable = False\n",
        "    for layer in densenet_model.layers:\n",
        "        layer.trainable = False\n",
        "    resnet_features = GlobalAveragePooling2D()(resnet_model(image_input))\n",
        "    densenet_features = GlobalAveragePooling2D()(densenet_model(image_input))\n",
        "    combined_features = Concatenate()([resnet_features, densenet_features])\n",
        "    x = Dropout(0.5)(combined_features)\n",
        "    x = Dense(256, activation=\"relu\")(combined_features)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
        "    return Model(inputs=image_input, outputs=output)\n",
        "\n",
        "previous_best_accuracy = 0.83\n",
        "if os.path.exists(checkpoint_drive_path):\n",
        "    print(\"Found checkpoint on Google Drive. Copying to local storage...\")\n",
        "    get_ipython().system(f'cp \"{checkpoint_drive_path}\" \"{local_checkpoint_path}\"')\n",
        "    from tensorflow.keras.models import load_model\n",
        "    try:\n",
        "        model_hybrid = load_model(local_checkpoint_path)\n",
        "        if len(model_hybrid.inputs) != 1 or model_hybrid.output_shape[-1] != num_classes:\n",
        "            print(\"Warning: Loaded model mismatch. Building new model...\")\n",
        "            model_hybrid = build_model(num_classes)\n",
        "        else:\n",
        "            print(\"Model loaded from checkpoint.\")\n",
        "            initial_val_loss, initial_val_accuracy = model_hybrid.evaluate(val_dataset, steps=validation_steps, verbose=2)\n",
        "            print(f\"Loaded model initial validation accuracy: {initial_val_accuracy:.4f}\")\n",
        "            if initial_val_accuracy < 0.80:  # If loaded accuracy is way off, rebuild\n",
        "                print(\"Warning: Loaded accuracy is too low. Building new model...\")\n",
        "                model_hybrid = build_model(num_classes)\n",
        "            else:\n",
        "                previous_best_accuracy = initial_val_accuracy  # Use actual loaded accuracy\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}. Building new model...\")\n",
        "        model_hybrid = build_model(num_classes)\n",
        "else:\n",
        "    print(\"No checkpoint found. Building a new model...\")\n",
        "    model_hybrid = build_model(num_classes)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)  # Constant, very small LR\n",
        "model_hybrid.compile(\n",
        "    optimizer=optimizer,\n",
        "    loss=\"categorical_crossentropy\",\n",
        "    metrics=[\"accuracy\"]\n",
        ")\n",
        "model_hybrid.summary()\n",
        "\n",
        "##############################################\n",
        "# 6. Evaluate Initial Model on Validation Set (If Not Loaded)\n",
        "##############################################\n",
        "if not os.path.exists(checkpoint_drive_path):\n",
        "    initial_val_loss, initial_val_accuracy = model_hybrid.evaluate(val_dataset, steps=validation_steps, verbose=2)\n",
        "    print(\"Initial validation accuracy: {:.4f}\".format(initial_val_accuracy))\n",
        "else:\n",
        "    initial_val_accuracy = previous_best_accuracy\n",
        "\n",
        "##############################################\n",
        "# 7. Train the Model with Callbacks\n",
        "##############################################\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=local_checkpoint_path,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "checkpoint_callback.best = previous_best_accuracy\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=10,  # Increased to give more time\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(f\"Training with previous best accuracy set to: {previous_best_accuracy:.4f}\")\n",
        "if TEST_MODE:\n",
        "    print(\"Running in TEST MODE...\")\n",
        "    import time\n",
        "    start_time = time.time()\n",
        "    history = model_hybrid.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_dataset,\n",
        "        validation_steps=validation_steps,\n",
        "        epochs=epochs,\n",
        "        verbose=1,\n",
        "        callbacks=[checkpoint_callback, early_stopping]\n",
        "    )\n",
        "    end_time = time.time()\n",
        "    print(f\"Test run completed in {end_time - start_time:.2f} seconds\")\n",
        "else:\n",
        "    print(\"Running in FULL TRAINING MODE...\")\n",
        "    history = model_hybrid.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_dataset,\n",
        "        validation_steps=validation_steps,\n",
        "        epochs=epochs,\n",
        "        verbose=1,\n",
        "        callbacks=[checkpoint_callback, early_stopping]\n",
        "    )\n",
        "\n",
        "new_best_accuracy = checkpoint_callback.best\n",
        "print(f\"New best validation accuracy: {new_best_accuracy:.4f}\")\n",
        "if new_best_accuracy > previous_best_accuracy:\n",
        "    print(\"New accuracy beats previous best. Copying checkpoint to Google Drive...\")\n",
        "    get_ipython().system(f'cp \"{local_checkpoint_path}\" \"{checkpoint_drive_path}\"')\n",
        "    print(\"Checkpoint saved to Google Drive.\")\n",
        "else:\n",
        "    print(\"New accuracy does not exceed previous best. Checkpoint not saved to Google Drive.\")\n",
        "\n",
        "##############################################\n",
        "# 8. Evaluate Model Performance\n",
        "##############################################\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "def get_predictions_and_labels(model, dataset, steps):\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "    for (images, labels) in dataset.take(steps):\n",
        "        batch_preds = model.predict(images)\n",
        "        preds.extend(np.argmax(batch_preds, axis=1))\n",
        "        true_labels.extend(np.argmax(labels, axis=1))\n",
        "    return np.array(preds), np.array(true_labels)\n",
        "\n",
        "preds, true_labels = get_predictions_and_labels(model_hybrid, val_dataset, validation_steps)\n",
        "cm = confusion_matrix(true_labels, preds)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "report = classification_report(true_labels, preds, target_names=label_encoder.classes_)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODh1BqBG2jxxHVcdKx3QS3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# version 5 : test_dataset accuracy\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Assuming label_encoder and model_hybrid are already defined from previous context\n",
        "# For this standalone version, we'll need to load them\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Load the pre-trained model\n",
        "checkpoint_path = '/content/drive/MyDrive/model_checkpoint.keras'\n",
        "if os.path.exists(checkpoint_path):\n",
        "    model_hybrid = tf.keras.models.load_model(checkpoint_path)\n",
        "    print('Model loaded successfully from checkpoint')\n",
        "else:\n",
        "    raise FileNotFoundError(f'Model checkpoint not found at {checkpoint_path}')\n",
        "\n",
        "# Load and prepare test data\n",
        "test_meta_file = '/content/SkinCancerDetection/test_metadata.csv'\n",
        "test_folder = '/content/SkinCancerDetection/test'\n",
        "test_df = pd.read_csv(test_meta_file)\n",
        "\n",
        "# Initialize and fit LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "test_labels = label_encoder.fit_transform(test_df['dx'])  # Fit on test data only for standalone\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Define data generator (assuming it's defined earlier, redefining minimally here)\n",
        "def load_and_preprocess_image(image_path, augment=False):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def data_generator(df, image_folder, labels, augment=False):\n",
        "    num_samples = len(df)\n",
        "    indices = np.arange(num_samples)\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        for start in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[start:start + batch_size]\n",
        "            batch_images = []\n",
        "            batch_labels = []\n",
        "            for idx in batch_indices:\n",
        "                raw_filename = str(df.iloc[idx]['image_id'])\n",
        "                filename = os.path.splitext(raw_filename)[0] + '.png'\n",
        "                image_path = os.path.join(image_folder, filename)\n",
        "                if not os.path.exists(image_path):\n",
        "                    print(f'Warning: {image_path} not found. Skipping.')\n",
        "                    continue\n",
        "                img = load_and_preprocess_image(image_path, augment=augment)\n",
        "                batch_images.append(img)\n",
        "                batch_labels.append(labels[idx])\n",
        "            if not batch_images:\n",
        "                continue\n",
        "            images_tensor = tf.stack(batch_images)\n",
        "            labels_tensor = tf.convert_to_tensor(tf.keras.utils.to_categorical(batch_labels, num_classes=num_classes), dtype=tf.float32)\n",
        "            yield images_tensor, labels_tensor\n",
        "\n",
        "# Define output signature\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(test_df, test_folder, test_labels, augment=False),\n",
        "    output_signature=output_signature\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "batch_size = 16  # Match training batch size\n",
        "test_steps = math.ceil(len(test_df) / batch_size)\n",
        "print(f'Test steps: {test_steps}')\n",
        "\n",
        "test_loss, test_accuracy = model_hybrid.evaluate(test_dataset, steps=test_steps, verbose=1)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "def get_predictions_and_labels(model, dataset, steps):\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "    for (images, labels) in dataset.take(steps):\n",
        "        batch_preds = model.predict(images)\n",
        "        preds.extend(np.argmax(batch_preds, axis=1))\n",
        "        true_labels.extend(np.argmax(labels, axis=1))\n",
        "    return np.array(preds), np.array(true_labels)\n",
        "\n",
        "test_preds, test_true_labels = get_predictions_and_labels(model_hybrid, test_dataset, test_steps)\n",
        "manual_accuracy = np.mean(test_preds == test_true_labels)\n",
        "print(f'Manually Calculated Test Accuracy: {manual_accuracy:.4f}')\n",
        "\n",
        "test_cm = confusion_matrix(test_true_labels, test_preds)\n",
        "print('\\nConfusion Matrix (Test):')\n",
        "print(test_cm)\n",
        "\n",
        "test_report = classification_report(test_true_labels, test_preds, target_names=label_encoder.classes_)\n",
        "print('\\nClassification Report (Test):')\n",
        "print(test_report)\n",
        "\n",
        "output_file = '/content/drive/MyDrive/test_evaluation_detailed2.txt'\n",
        "with open(output_file, 'w') as f:\n",
        "    f.write(f'Test Loss: {test_loss:.4f}\\n')\n",
        "    f.write(f'Test Accuracy: {test_accuracy:.4f}\\n')\n",
        "    f.write(f'Manually Calculated Test Accuracy: {manual_accuracy:.4f}\\n')\n",
        "    f.write('\\nConfusion Matrix (Test):\\n')\n",
        "    f.write(np.array2string(test_cm))\n",
        "    f.write('\\n\\nClassification Report (Test):\\n')\n",
        "    f.write(test_report)\n",
        "print(f'Test evaluation results saved to {output_file}')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9t0cWKwl3_nm",
        "executionInfo": {
          "status": "ok",
          "timestamp": 1740330275629,
          "user_tz": -330,
          "elapsed": 18949,
          "user": {
            "displayName": "SmartBin",
            "userId": "04000854669842934269"
          }
        },
        "outputId": "87a56236-1061-417d-f3da-3e99de85bb81"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Model loaded successfully from checkpoint\n",
            "Test steps: 33\n",
            "\u001b[1m33/33\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m9s\u001b[0m 252ms/step - accuracy: 0.8382 - loss: 0.6972\n",
            "Test Loss: 0.7411\n",
            "Test Accuracy: 0.8305\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 156ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 151ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 140ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 153ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 142ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 150ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 160ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 143ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 139ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 157ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 138ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 145ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 159ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 146ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 149ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 141ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 137ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 144ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 147ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 180ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 197ms/step\n",
            "\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 126ms/step\n",
            "Manually Calculated Test Accuracy: 0.8305\n",
            "\n",
            "Confusion Matrix (Test):\n",
            "[[64  1  4  1  3  1  1]\n",
            " [ 3 62  5  0  3  2  0]\n",
            " [ 2  4 54  2 10  3  0]\n",
            " [ 0  1  0 74  0  0  0]\n",
            " [ 3  2  9  0 56  2  3]\n",
            " [ 1  2  4  0 15 52  1]\n",
            " [ 0  1  0  0  0  0 74]]\n",
            "\n",
            "Classification Report (Test):\n",
            "              precision    recall  f1-score   support\n",
            "\n",
            "       akiec       0.88      0.85      0.86        75\n",
            "         bcc       0.85      0.83      0.84        75\n",
            "         bkl       0.71      0.72      0.72        75\n",
            "          df       0.96      0.99      0.97        75\n",
            "         mel       0.64      0.75      0.69        75\n",
            "          nv       0.87      0.69      0.77        75\n",
            "        vasc       0.94      0.99      0.96        75\n",
            "\n",
            "    accuracy                           0.83       525\n",
            "   macro avg       0.83      0.83      0.83       525\n",
            "weighted avg       0.83      0.83      0.83       525\n",
            "\n",
            "Test evaluation results saved to /content/drive/MyDrive/test_evaluation_detailed2.txt\n"
          ]
        }
      ]
    }
  ]
}
