{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyODh1BqBG2jxxHVcdKx3QS3"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "code",
      "metadata": {
        "id": "3GBjrWyCxOCd"
      },
      "source": [
        "# Skin Cancer Detection - Version 5: Training (84% Validation Accuracy)\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from tensorflow.keras.applications.resnet50 import preprocess_input as resnet_preprocess\n",
        "from tensorflow.keras.utils import to_categorical\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from tensorflow.keras.layers import Input, Dense, GlobalAveragePooling2D, Dropout, Concatenate\n",
        "from tensorflow.keras.models import Model\n",
        "from tensorflow.keras.applications import ResNet50, DenseNet121\n",
        "from tensorflow.keras.callbacks import ModelCheckpoint, EarlyStopping\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "\n",
        "# 1. Setup: Mount Drive, Copy Data to Local Storage & Verify Paths\n",
        "drive.mount('/content/drive')\n",
        "drive_base_path = \"/content/drive/MyDrive/SkinCancerDetection\"\n",
        "local_base_path = \"/content/SkinCancerDetection\"\n",
        "\n",
        "if not os.path.exists(local_base_path):\n",
        "    print(\"Copying dataset from Google Drive to local storage...\")\n",
        "    os.system(f'cp -r \"{drive_base_path}\" \"{local_base_path}\"')\n",
        "else:\n",
        "    print(\"Dataset already exists in local storage.\")\n",
        "\n",
        "base_path = local_base_path\n",
        "print(f\"Using base path: {base_path}\")\n",
        "\n",
        "train_meta_file = os.path.join(base_path, \"train_metadata.csv\")\n",
        "val_meta_file = os.path.join(base_path, \"val_metadata.csv\")\n",
        "test_meta_file = os.path.join(base_path, \"test_metadata.csv\")\n",
        "\n",
        "for file in [train_meta_file, val_meta_file, test_meta_file]:\n",
        "    if not os.path.exists(file):\n",
        "        raise FileNotFoundError(f\"Metadata file not found: {file}\")\n",
        "    else:\n",
        "        print(f\"Found metadata file: {file}\")\n",
        "\n",
        "train_folder = os.path.join(base_path, \"train\")\n",
        "val_folder = os.path.join(base_path, \"val\")\n",
        "test_folder = os.path.join(base_path, \"test\")\n",
        "\n",
        "for folder in [train_folder, val_folder, test_folder]:\n",
        "    if not os.path.exists(folder):\n",
        "        raise FileNotFoundError(f\"Image folder not found: {folder}\")\n",
        "    else:\n",
        "        print(f\"Found image folder: {folder}\")\n",
        "\n",
        "# 2. Load & Preprocess Metadata\n",
        "def load_metadata(csv_path):\n",
        "    df = pd.read_csv(csv_path)\n",
        "    required_columns = ['image_id', 'dx']\n",
        "    missing = [col for col in required_columns if col not in df.columns]\n",
        "    if missing:\n",
        "        raise ValueError(f\"Missing columns in {csv_path}: {missing}\")\n",
        "    return df\n",
        "\n",
        "train_df = load_metadata(train_meta_file)\n",
        "val_df = load_metadata(val_meta_file)\n",
        "test_df = load_metadata(test_meta_file)\n",
        "\n",
        "TEST_MODE = False\n",
        "if TEST_MODE:\n",
        "    train_df = train_df.sample(100, random_state=42)\n",
        "    val_df = val_df.sample(50, random_state=42)\n",
        "    batch_size = 8\n",
        "    epochs = 2\n",
        "else:\n",
        "    batch_size = 32\n",
        "    epochs = 20\n",
        "\n",
        "label_encoder = LabelEncoder()\n",
        "train_labels = label_encoder.fit_transform(train_df['dx'])\n",
        "val_labels = label_encoder.transform(val_df['dx'])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "print(\"Number of classes:\", num_classes)\n",
        "\n",
        "# 3. Image Loading & Preprocessing\n",
        "def load_and_preprocess_image(image_path, augment=False):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    if augment:\n",
        "        img = tf.image.random_flip_left_right(img)\n",
        "        img = tf.image.random_brightness(img, max_delta=0.1)\n",
        "        img = tf.image.random_contrast(img, lower=0.9, upper=1.1)\n",
        "    img = resnet_preprocess(img)\n",
        "    return img\n",
        "\n",
        "# 4. Data Generator\n",
        "def data_generator(df, image_folder, labels, augment=False):\n",
        "    num_samples = len(df)\n",
        "    indices = np.arange(num_samples)\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        for start in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[start:start + batch_size]\n",
        "            batch_images = []\n",
        "            batch_labels = []\n",
        "            for idx in batch_indices:\n",
        "                try:\n",
        "                    raw_filename = str(df.iloc[idx]['image_id'])\n",
        "                    filename = os.path.splitext(raw_filename)[0] + \".png\"\n",
        "                    image_path = os.path.join(image_folder, filename)\n",
        "                    if not os.path.exists(image_path):\n",
        "                        print(f\"Warning: {image_path} not found. Skipping.\")\n",
        "                        continue\n",
        "                    img = load_and_preprocess_image(image_path, augment=augment)\n",
        "                    batch_images.append(img)\n",
        "                    batch_labels.append(labels[idx])\n",
        "                except Exception as e:\n",
        "                    print(f\"Error processing index {idx}: {e}\")\n",
        "                    continue\n",
        "            if not batch_images:\n",
        "                continue\n",
        "            images_tensor = tf.stack(batch_images)\n",
        "            labels_tensor = tf.convert_to_tensor(to_categorical(batch_labels, num_classes=num_classes), dtype=tf.float32)\n",
        "            yield images_tensor, labels_tensor\n",
        "\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "train_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(train_df, train_folder, train_labels, augment=True),\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=([None, 224, 224, 3], [None, num_classes])\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "val_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(val_df, val_folder, val_labels, augment=False),\n",
        "    output_types=(tf.float32, tf.float32),\n",
        "    output_shapes=([None, 224, 224, 3], [None, num_classes])\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "steps_per_epoch = math.ceil(len(train_df) / batch_size)\n",
        "validation_steps = math.ceil(len(val_df) / batch_size)\n",
        "print(f\"Steps per epoch (train): {steps_per_epoch}\")\n",
        "print(f\"Steps per epoch (val): {validation_steps}\")\n",
        "\n",
        "# 5. Build the ResNet50 + DenseNet121 Hybrid Model\n",
        "local_checkpoint_path = \"model_checkpoint.keras\"\n",
        "checkpoint_drive_path = \"/content/drive/MyDrive/model_checkpoint.keras\"\n",
        "\n",
        "def build_model(num_classes):\n",
        "    image_input = Input(shape=(224, 224, 3), name=\"image_input\")\n",
        "    resnet_model = ResNet50(include_top=False, weights=\"imagenet\", name=\"resnet50\")\n",
        "    densenet_model = DenseNet121(include_top=False, weights=\"imagenet\", name=\"densenet121\")\n",
        "    for layer in resnet_model.layers:\n",
        "        layer.trainable = False\n",
        "    for layer in densenet_model.layers:\n",
        "        layer.trainable = False\n",
        "    resnet_features = GlobalAveragePooling2D()(resnet_model(image_input))\n",
        "    densenet_features = GlobalAveragePooling2D()(densenet_model(image_input))\n",
        "    combined_features = Concatenate()([resnet_features, densenet_features])\n",
        "    x = Dropout(0.5)(combined_features)\n",
        "    x = Dense(256, activation=\"relu\")(combined_features)\n",
        "    x = Dropout(0.5)(x)\n",
        "    output = Dense(num_classes, activation=\"softmax\", name=\"output\")(x)\n",
        "    return Model(inputs=image_input, outputs=output)\n",
        "\n",
        "previous_best_accuracy = 0.83\n",
        "if os.path.exists(checkpoint_drive_path):\n",
        "    print(\"Found checkpoint on Google Drive. Copying to local storage...\")\n",
        "    os.system(f'cp \"{checkpoint_drive_path}\" \"{local_checkpoint_path}\"')\n",
        "    try:\n",
        "        model_hybrid = tf.keras.models.load_model(local_checkpoint_path)\n",
        "        if len(model_hybrid.inputs) != 1 or model_hybrid.output_shape[-1] != num_classes:\n",
        "            print(\"Warning: Loaded model mismatch. Building new model...\")\n",
        "            model_hybrid = build_model(num_classes)\n",
        "        else:\n",
        "            print(\"Model loaded from checkpoint.\")\n",
        "            initial_val_loss, initial_val_accuracy = model_hybrid.evaluate(val_dataset, steps=validation_steps, verbose=2)\n",
        "            print(f\"Loaded model initial validation accuracy: {initial_val_accuracy:.4f}\")\n",
        "            if initial_val_accuracy < 0.80:\n",
        "                print(\"Warning: Loaded accuracy is too low. Building new model...\")\n",
        "                model_hybrid = build_model(num_classes)\n",
        "            else:\n",
        "                previous_best_accuracy = initial_val_accuracy\n",
        "    except Exception as e:\n",
        "        print(f\"Error loading checkpoint: {e}. Building new model...\")\n",
        "        model_hybrid = build_model(num_classes)\n",
        "else:\n",
        "    print(\"No checkpoint found. Building a new model...\")\n",
        "    model_hybrid = build_model(num_classes)\n",
        "\n",
        "optimizer = tf.keras.optimizers.Adam(learning_rate=5e-6)\n",
        "model_hybrid.compile(optimizer=optimizer, loss=\"categorical_crossentropy\", metrics=[\"accuracy\"])\n",
        "model_hybrid.summary()\n",
        "\n",
        "# 6. Evaluate Initial Model\n",
        "if not os.path.exists(checkpoint_drive_path):\n",
        "    initial_val_loss, initial_val_accuracy = model_hybrid.evaluate(val_dataset, steps=validation_steps, verbose=2)\n",
        "    print(\"Initial validation accuracy: {:.4f}\".format(initial_val_accuracy))\n",
        "else:\n",
        "    initial_val_accuracy = previous_best_accuracy\n",
        "\n",
        "# 7. Train the Model\n",
        "checkpoint_callback = ModelCheckpoint(\n",
        "    filepath=local_checkpoint_path,\n",
        "    monitor='val_accuracy',\n",
        "    save_best_only=True,\n",
        "    save_weights_only=False,\n",
        "    mode='max',\n",
        "    verbose=1\n",
        ")\n",
        "checkpoint_callback.best = previous_best_accuracy\n",
        "\n",
        "early_stopping = EarlyStopping(\n",
        "    monitor='val_accuracy',\n",
        "    patience=10,\n",
        "    verbose=1,\n",
        "    restore_best_weights=True\n",
        ")\n",
        "\n",
        "print(f\"Training with previous best accuracy set to: {previous_best_accuracy:.4f}\")\n",
        "if TEST_MODE:\n",
        "    print(\"Running in TEST MODE...\")\n",
        "    history = model_hybrid.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_dataset,\n",
        "        validation_steps=validation_steps,\n",
        "        epochs=epochs,\n",
        "        verbose=1,\n",
        "        callbacks=[checkpoint_callback, early_stopping]\n",
        "    )\n",
        "else:\n",
        "    print(\"Running in FULL TRAINING MODE...\")\n",
        "    history = model_hybrid.fit(\n",
        "        train_dataset,\n",
        "        steps_per_epoch=steps_per_epoch,\n",
        "        validation_data=val_dataset,\n",
        "        validation_steps=validation_steps,\n",
        "        epochs=epochs,\n",
        "        verbose=1,\n",
        "        callbacks=[checkpoint_callback, early_stopping]\n",
        "    )\n",
        "\n",
        "new_best_accuracy = checkpoint_callback.best\n",
        "print(f\"New best validation accuracy: {new_best_accuracy:.4f}\")\n",
        "if new_best_accuracy > previous_best_accuracy:\n",
        "    print(\"New accuracy beats previous best. Copying checkpoint to Google Drive...\")\n",
        "    os.system(f'cp \"{local_checkpoint_path}\" \"{checkpoint_drive_path}\"')\n",
        "    print(\"Checkpoint saved to Google Drive.\")\n",
        "else:\n",
        "    print(\"New accuracy does not exceed previous best. Checkpoint not saved to Google Drive.\")\n",
        "\n",
        "# 8. Evaluate Model Performance\n",
        "def get_predictions_and_labels(model, dataset, steps):\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "    for (images, labels) in dataset.take(steps):\n",
        "        batch_preds = model.predict(images)\n",
        "        preds.extend(np.argmax(batch_preds, axis=1))\n",
        "        true_labels.extend(np.argmax(labels, axis=1))\n",
        "    return np.array(preds), np.array(true_labels)\n",
        "\n",
        "preds, true_labels = get_predictions_and_labels(model_hybrid, val_dataset, validation_steps)\n",
        "cm = confusion_matrix(true_labels, preds)\n",
        "print(\"Confusion Matrix:\")\n",
        "print(cm)\n",
        "report = classification_report(true_labels, preds, target_names=label_encoder.classes_)\n",
        "print(\"Classification Report:\")\n",
        "print(report)"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "9t0cWKwl3_nm"
      },
      "source": [
        "# Skin Cancer Detection - Version 5: Test Dataset Evaluation\n",
        "\n",
        "import os\n",
        "import math\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import tensorflow as tf\n",
        "from sklearn.metrics import confusion_matrix, classification_report\n",
        "from google.colab import drive\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Mount Google Drive\n",
        "if not os.path.exists('/content/drive'):\n",
        "    drive.mount('/content/drive')\n",
        "\n",
        "# Load the pre-trained model\n",
        "checkpoint_path = '/content/drive/MyDrive/model_checkpoint.keras'\n",
        "if os.path.exists(checkpoint_path):\n",
        "    model_hybrid = tf.keras.models.load_model(checkpoint_path)\n",
        "    print('Model loaded successfully from checkpoint')\n",
        "else:\n",
        "    raise FileNotFoundError(f'Model checkpoint not found at {checkpoint_path}')\n",
        "\n",
        "# Load and prepare test data\n",
        "test_meta_file = '/content/SkinCancerDetection/test_metadata.csv'\n",
        "test_folder = '/content/SkinCancerDetection/test'\n",
        "test_df = pd.read_csv(test_meta_file)\n",
        "\n",
        "# Initialize and fit LabelEncoder\n",
        "label_encoder = LabelEncoder()\n",
        "test_labels = label_encoder.fit_transform(test_df['dx'])\n",
        "num_classes = len(label_encoder.classes_)\n",
        "\n",
        "# Define data preprocessing and generator\n",
        "def load_and_preprocess_image(image_path, augment=False):\n",
        "    img = tf.io.read_file(image_path)\n",
        "    img = tf.image.decode_jpeg(img, channels=3)\n",
        "    img = tf.image.resize(img, (224, 224))\n",
        "    img = tf.keras.applications.resnet50.preprocess_input(img)\n",
        "    return img\n",
        "\n",
        "def data_generator(df, image_folder, labels, augment=False):\n",
        "    num_samples = len(df)\n",
        "    indices = np.arange(num_samples)\n",
        "    while True:\n",
        "        np.random.shuffle(indices)\n",
        "        for start in range(0, num_samples, batch_size):\n",
        "            batch_indices = indices[start:start + batch_size]\n",
        "            batch_images = []\n",
        "            batch_labels = []\n",
        "            for idx in batch_indices:\n",
        "                raw_filename = str(df.iloc[idx]['image_id'])\n",
        "                filename = os.path.splitext(raw_filename)[0] + '.png'\n",
        "                image_path = os.path.join(image_folder, filename)\n",
        "                if not os.path.exists(image_path):\n",
        "                    print(f'Warning: {image_path} not found. Skipping.')\n",
        "                    continue\n",
        "                img = load_and_preprocess_image(image_path, augment=augment)\n",
        "                batch_images.append(img)\n",
        "                batch_labels.append(labels[idx])\n",
        "            if not batch_images:\n",
        "                continue\n",
        "            images_tensor = tf.stack(batch_images)\n",
        "            labels_tensor = tf.convert_to_tensor(tf.keras.utils.to_categorical(batch_labels, num_classes=num_classes), dtype=tf.float32)\n",
        "            yield images_tensor, labels_tensor\n",
        "\n",
        "# Define output signature\n",
        "output_signature = (\n",
        "    tf.TensorSpec(shape=(None, 224, 224, 3), dtype=tf.float32),\n",
        "    tf.TensorSpec(shape=(None, num_classes), dtype=tf.float32)\n",
        ")\n",
        "\n",
        "test_dataset = tf.data.Dataset.from_generator(\n",
        "    lambda: data_generator(test_df, test_folder, test_labels, augment=False),\n",
        "    output_signature=output_signature\n",
        ").prefetch(tf.data.AUTOTUNE)\n",
        "\n",
        "batch_size = 16\n",
        "test_steps = math.ceil(len(test_df) / batch_size)\n",
        "print(f'Test steps: {test_steps}')\n",
        "\n",
        "# Evaluate on test dataset\n",
        "test_loss, test_accuracy = model_hybrid.evaluate(test_dataset, steps=test_steps, verbose=1)\n",
        "print(f'Test Loss: {test_loss:.4f}')\n",
        "print(f'Test Accuracy: {test_accuracy:.4f}')\n",
        "\n",
        "# Get predictions and labels\n",
        "def get_predictions_and_labels(model, dataset, steps):\n",
        "    preds = []\n",
        "    true_labels = []\n",
        "    for (images, labels) in dataset.take(steps):\n",
        "        batch_preds = model.predict(images)\n",
        "        preds.extend(np.argmax(batch_preds, axis=1))\n",
        "        true_labels.extend(np.argmax(labels, axis=1))\n",
        "    return np.array(preds), np.array(true_labels)\n",
        "\n",
        "test_preds, test_true_labels = get_predictions_and_labels(model_hybrid, test_dataset, test_steps)\n",
        "manual_accuracy = np.mean(test_preds == test_true_labels)\n",
        "print(f'Manually Calculated Test Accuracy: {manual_accuracy:.4f}')\n",
        "\n",
        "# Compute and print confusion matrix and classification report\n",
        "test_cm = confusion_matrix(test_true_labels, test_preds)\n",
        "print('\\nConfusion Matrix (Test):')\n",
        "print(test_cm)\n",
        "\n",
        "test_report = classification_report(test_true_labels, test_preds, target_names=label_encoder.classes_)\n",
        "print('\\nClassification Report (Test):')\n",
        "print(test_report)\n",
        "\n",
        "# Save results to file\n",
        "output_file = '/content/drive/MyDrive/test_evaluation_detailed2.txt'\n",
        "with open(output_file, 'w') as f:\n",
        "    f.write(f'Test Loss: {test_loss:.4f}\\n')\n",
        "    f.write(f'Test Accuracy: {test_accuracy:.4f}\\n')\n",
        "    f.write(f'Manually Calculated Test Accuracy: {manual_accuracy:.4f}\\n')\n",
        "    f.write('\\nConfusion Matrix (Test):\\n')\n",
        "    f.write(np.array2string(test_cm))\n",
        "    f.write('\\n\\nClassification Report (Test):\\n')\n",
        "    f.write(test_report)\n",
        "print(f'Test evaluation results saved to {output_file}')"
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}
